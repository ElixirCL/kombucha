# Kombucha Programming Language

```elixir
Mix.install([
  {:kino, "~> 0.10.0"}
])
```

<!-- livebook:{"output":true} -->

```
Resolving Hex dependencies...
Resolution completed in 0.078s
New:
  kino 0.10.0
  table 0.1.2
* Getting kino (Hex package)
* Getting table (Hex package)
==> table
Compiling 5 files (.ex)
Generated table app
==> kino
Compiling 41 files (.ex)
Generated kino app
```

<!-- livebook:{"output":true} -->

```
:ok
```

## Kombucha Transpiler

```elixir
defmodule Kombucha do
  defp peek(tokens, count \\ 1) do
    Enum.take(tokens, count)
  end

  defp next([]) do
    {[], []}
  end

  defp next(tokens) do
    {peek(tokens), Enum.drop(tokens, 1)}
  end

  defp next_token(tokens) do
    case next(tokens) do
      {[], []} -> {{:eof, nil}, []}
      {[item], remaining_tokens} -> {item, remaining_tokens}
    end
  end

  defp process_const(acc, _rest, tokens) do
    # Transform const to a public function
    # const my_const {
    #  "value"
    # }
    # def my_const do 
    #  "value"
    # end
    #
    # const my_const = 1324
    # def my_const, do: 1234

    {{name, _index}, remain} = next_token(tokens)
    {{value, _index}, remain} = next_token(remain)

    case value do
      "=" <> _rest -> {acc ++ ["def " <> String.trim(name) <> ", do: "], remain}
      value -> {acc ++ ["def " <> String.trim(name) <> " " <> value], remain}
    end
  end

  def process_comment(acc, tokens) do
    {token, remain} = next_token(tokens)

    case token do
      {:eof, _} ->
        {acc, []}

      # Go back to the main flow
      {"__comment_block_end__" <> _rest, _} ->
        {acc, remain}

      # Keep consuming tokens until block end or eof
      _any ->
        process_comment(acc, remain)
    end
  end

  defp process_struct(acc, tokens) do
    # Transform structs to the elixir format
    {{name, _}, remain} = next_token(tokens)
    {{_do, _}, remain} = next_token(remain)
    {{_newline, _}, remain} = next_token(remain)
    {{fields, _}, remain} = next_token(remain)

    {acc ++
       [
         """
         defmodule #{String.trim(name)} do
           alias __MODULE__
           defstruct #{String.trim(fields)}
         """
       ], remain}
  end

  defp process_lambda(acc, rest, tokens) do
    {acc ++ [String.replace_trailing(rest, "(", ".(")], tokens}
  end

  defp transform(acc, []) do
    acc
  end

  defp transform(acc, tokens) do
    # Get the next token
    {{token, _index}, remaining_tokens} = next_token(tokens)

    # Return the accumulator and remaining tokens
    {out, remaining_tokens} =
      case token do
        # Keywords
        "module" <> rest -> {acc ++ ["defmodule" <> rest], remaining_tokens}
        "macro" <> rest -> {acc ++ ["defmacro" <> rest], remaining_tokens}
        "guard" <> rest -> {acc ++ ["defguard" <> rest], remaining_tokens}
        "struct" <> _rest -> process_struct(acc, remaining_tokens)
        "pub" <> rest -> {acc ++ ["def" <> rest], remaining_tokens}
        "fun" <> rest -> {acc ++ ["defp" <> rest], remaining_tokens}
        "$" <> rest -> process_lambda(acc, rest, remaining_tokens)
        "const" <> rest -> process_const(acc, rest, remaining_tokens)
        "__comment_block_start__" <> _rest -> process_comment(acc, remaining_tokens)
        "};" <> rest -> {acc ++ ["}" <> rest], remaining_tokens}
        # End the transpilation
        token when token == :eof -> {acc, []}
        # Store characters that are not keywords
        token -> {acc ++ [token], remaining_tokens}
      end

    # Traverse the token tree until :eof
    transform(out, remaining_tokens)
  end

  defp tokenize(input) do
    input
    |> String.trim()
    |> String.replace("\r", "\n")
    |> String.replace("\r\n", "\n")
    |> String.replace("{\n", "do\n")
    # lambda functions
    |> String.replace("{|", "fn")
    |> String.replace("}\n", "end\n")
    # for maps, structs
    |> String.replace("};", " };")
    |> String.replace("/*", " __comment_block_start__ ")
    |> String.replace("*/", " __comment_block_end__ ")
    |> String.replace("\n", " \n ")
    |> String.split(~r/(?<=[()\s;=+\-*\/]|[()])/)
    |> Enum.filter(&(&1 != "" && &1 != " "))
    |> Enum.with_index()
  end

  def transpile(input) do
    transform([], tokenize(input))
    |> Enum.join()
  end

  def print(input) do
    transpile(input)
    |> Kino.Text.new()
  end

  def eval(input) do
    transpile(input)
    |> Code.eval_string()
    |> then(fn {result, _} -> result end)
  end
end
```

<!-- livebook:{"output":true} -->

```
{:module, Kombucha, <<70, 79, 82, 49, 0, 0, 32, ...>>, {:eval, 1}}
```

## Modules

```elixir
"""
module Brew {
  fun message() {
    "Strong Kombucha"
  }

  pub strong() {
    message()
  }
}
Brew.strong()
"""
|> Kombucha.eval()
```

<!-- livebook:{"output":true} -->

```
"Strong Kombucha"
```

## Lambdas

```elixir
"""
hey = {| message -> 
  to_string(message)
  |> String.upcase()
}
$hey(:ho)
"""
|> Kombucha.eval()
```

<!-- livebook:{"output":true} -->

```
"HO"
```

## Tuples

```elixir
"""
/* needs a semicolon at the end to differentiate from "end" blocks */
{:ok, 1234};
"""
|> Kombucha.eval()
```

<!-- livebook:{"output":true} -->

```
{:ok, 1234}
```

## Const

```elixir
"""
module MyModule {
  const myconst = 123
}

MyModule.myconst
"""
|> Kombucha.eval()
```

<!-- livebook:{"output":true} -->

```
123
```

```elixir
"""
/* This is a comment */
module Hello {
  const world = "
  Hello Kombucha
  World
  "
  /* 
  This is another 
  comment 
  */

  const drink {
    "Drink 500 Pints of Kombucha"
  }
}

Hello.drink
"""
|> Kombucha.eval()
```

<!-- livebook:{"output":true} -->

```
"Drink 500 Pints of Kombucha"
```

## Structs

```elixir
"""
struct Glass {
  ~w[liquid]a
  pub new() {
    %Glass{liquid: :kombucha};
  }
}
Glass.new()
"""
|> Kombucha.eval()
```

<!-- livebook:{"output":true} -->

```
%Glass{liquid: :kombucha}
```
